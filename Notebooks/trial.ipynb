{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndkGl3N_u3nl",
    "outputId": "6f476a6a-3e6c-414f-da8e-bfc3b18fa88f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21613 entries, 7129300520 to 1523300157\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           21613 non-null  object \n",
      " 1   zipcode        21613 non-null  int64  \n",
      " 2   lat            21613 non-null  float64\n",
      " 3   long           21613 non-null  float64\n",
      " 4   bedrooms       21613 non-null  int64  \n",
      " 5   bathrooms      21613 non-null  float64\n",
      " 6   sqft_above     21613 non-null  int64  \n",
      " 7   sqft_basement  21613 non-null  int64  \n",
      " 8   sqft_lot       21613 non-null  int64  \n",
      " 9   floors         21613 non-null  float64\n",
      " 10  waterfront     21613 non-null  int64  \n",
      " 11  condition      21613 non-null  int64  \n",
      " 12  yr_built       21613 non-null  int64  \n",
      " 13  yr_renovated   21613 non-null  int64  \n",
      " 14  price          21613 non-null  int64  \n",
      "dtypes: float64(4), int64(10), object(1)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "## NAME: Laila Ali Yassin Cruz\n",
    "# Assignment 1\n",
    "\n",
    "# Importing the data #\n",
    "import pandas as pd\n",
    "path = 'https://raw.githubusercontent.com/cinnData/MLearning/main/Data/'\n",
    "df = pd.read_csv(path + 'king.csv', index_col=0)\n",
    "\n",
    "# Exploring the data #\n",
    "df.info()\n",
    "df.index.duplicated().sum()\n",
    "df.duplicated().sum()\n",
    "duplicates = df.index[df.index.duplicated()]\n",
    "df.loc[duplicates].head()\n",
    "df['price'] = df['price']/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfqFNVFZw_lF"
   },
   "source": [
    "## Q1. The role of longitude and latitude in the prediction of real estate prices is unclear. Do they really contribute to get better predictions in the first model of this example? If we keep them in the second model, do we get a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1DoIrJlw5bn",
    "outputId": "e782851f-749a-4240-eb81-7f5c9b6411d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.646"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We obtain the first model's prediction strength including latitude and longitute\n",
    "\n",
    "# Linear regression equation #\n",
    "y = df.iloc[:, -1]\n",
    "X_model1 = df.iloc[:, 2:-1]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_model1, y)\n",
    "y_pred_model1 = reg.predict(X_model1)\n",
    "reg.score(X_model1, y).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOciBsEDxVF7",
    "outputId": "7d9b7422-9f70-4021-fb7f-bb01dde07171"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.587"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We obtain the first model's prediction strength removing latitude and longitute\n",
    "\n",
    "# Linear regression equation\n",
    "y = df.iloc[:, -1]\n",
    "X_2 = df.iloc[:, 4:-1] # we start the dataframe from the fourth column to remove\n",
    "reg.fit(X_2, y)\n",
    "y_pred_2 = reg.predict(X_2)\n",
    "reg.score(X_2, y).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_963r85vyU7W",
    "outputId": "28741261-232c-4eed-b558-e2625afddf5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.786"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now do the same comparison with the second model, using zipcodes as dummies\n",
    "\n",
    "# Q4. Dummies for the zipcodes #\n",
    "X_model2_1 = df.iloc[:, 2:-1]\n",
    "X_model2_2 = pd.get_dummies(df['zipcode'])\n",
    "X_model2_2.head()\n",
    "X_model2 = pd.concat([X_model2_1, X_model2_2], axis=1)\n",
    "X_model2.shape\n",
    "X_model2 = X_model2.values\n",
    "reg.fit(X_model2, y)\n",
    "y_pred_model2 = reg.predict(X_model2)\n",
    "reg.score(X_model2, y).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdxha_CSzASB",
    "outputId": "c08a80f4-2b86-41af-be15-26a8f0dd3937"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now do the same comparison with the second model, but removing latitude and longitude\n",
    "\n",
    "# Q4. Dummies for the zipcodes #\n",
    "X1 = df.iloc[:, 4:-1]  #we start the dataframe from the fourth column to remove lat and long\n",
    "X2 = pd.get_dummies(df['zipcode'])\n",
    "X2.head()\n",
    "X = pd.concat([X1, X2], axis=1)\n",
    "X.shape\n",
    "X = X.values\n",
    "reg.fit(X, y)\n",
    "y_pred = reg.predict(X)\n",
    "reg.score(X, y).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO4RU_P2yaqx"
   },
   "source": [
    "## **Answer Q1:**\n",
    "We're seeing that while for the first model the prediction score deteriorates from 64.6% to 58.7% by removing latitude and longitude,once we start using the zipcodes as dummies the impact of removing latitude and longitude in the model just makes the model lose prediction in 0.1%, going from 78.6% to 78.5%). This may be because the effect of lat and longitude is already being captured in a great part by zipcode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3xvPdIH0GjI"
   },
   "source": [
    "## Q2. Evaluate in dollar terms the predictive performance of the two models presented in this example. For instance, you can use the mean (or median) absolute error. Can you make a statement like \"the value of x% of the houses can be predicted with an error below y thousand dollars\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJ2xWZlQ0JVl"
   },
   "outputs": [],
   "source": [
    "# We add the predicted prices in our dataframe\n",
    "df1 =df\n",
    "df1['y_pred_model1'] = y_pred_model1\n",
    "df1['y_pred_model2'] = y_pred_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIVZ8nKs1lp1"
   },
   "outputs": [],
   "source": [
    "# We obtain the model's errors\n",
    "df1['error_model1'] = abs(df1['y_pred_model1']- df1['price'])\n",
    "df1['error_model2'] = abs(df1['y_pred_model2']- df1['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2kED-2_3hBk"
   },
   "source": [
    "## **Answer Q2: Model 1**\n",
    "We can see that in error terms, the value of 50% of the houses will be predicted with an error below 97,000 dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF4uPC_C3AUo",
    "outputId": "cdc39f3b-c694-43c8-b8aa-e9c936a02406"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21613.000000\n",
       "mean       139.328120\n",
       "std        168.191617\n",
       "min          0.018830\n",
       "25%         45.393321\n",
       "50%         97.696843\n",
       "75%        177.025142\n",
       "max       3922.773169\n",
       "Name: error_model1, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['error_model1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV2fz6wd37Cq"
   },
   "source": [
    "## **Answer Q2: Model 2**\n",
    "We can see that in error terms, the value of 50% of the houses will be predicted with an error below 70,000 dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIH2JlKO3fRZ",
    "outputId": "ebd9c6b9-2e50-452f-f69a-fcbef7a876c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21613.000000\n",
       "mean       103.057215\n",
       "std        135.093873\n",
       "min          0.006001\n",
       "25%         32.893924\n",
       "50%         70.611755\n",
       "75%        126.538218\n",
       "max       4207.544880\n",
       "Name: error_model2, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['error_model2'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHL4pJfs3__V"
   },
   "source": [
    "## Q3. Is it better to use the percentage error in the above assessment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdxHTVVT4IVK"
   },
   "outputs": [],
   "source": [
    "# We obtain the model's errors as a percentage of of the real price\n",
    "df1['error_model1_perc'] = df1['error_model1']/ df1['price']\n",
    "\n",
    "df1['error_model2_perc'] = df1['error_model2']/ df1['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGKax-cL4gHj"
   },
   "source": [
    "## **Answer Q3: Model 1**\n",
    "We can see that in percentage terms, the value of 50% of the houses will be predicted with an error below 20% of their actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZRHdEpM4Uho",
    "outputId": "fc2dd597-57fb-441a-e1b5-3114dbebca5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21613.000000\n",
       "mean         0.279175\n",
       "std          0.251885\n",
       "min          0.000051\n",
       "25%          0.100224\n",
       "50%          0.216834\n",
       "75%          0.380352\n",
       "max          3.254671\n",
       "Name: error_model1_perc, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['error_model1_perc'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl49YaNx4nIg"
   },
   "source": [
    "## **Answer Q3: Model 2**\n",
    "We can see that in percentage terms, the value of 50% of the houses will be predicted with an error below 15% of their actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHLJ_MDS4bXH",
    "outputId": "af2900e2-d5bf-490f-9685-33e001ad2cf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21613.000000\n",
       "mean         0.208424\n",
       "std          0.199224\n",
       "min          0.000012\n",
       "25%          0.071493\n",
       "50%          0.154808\n",
       "75%          0.284807\n",
       "max          3.166680\n",
       "Name: error_model2_perc, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['error_model2_perc'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_PFEZT14p5H"
   },
   "source": [
    "In this instance, since the value of a house is very different from one to the other, seeing the errors in percentage points helps us see the magnitudes of the errors in relative terms to the price of the house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v20P12MM41bn"
   },
   "source": [
    "## Q4. Can the strong correlation be an artifact created by the extreme values? Trim the data set, dropping the houses beyond a certain threshold of price and/or size. Do you get a better model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBuy7Eh15GBW",
    "outputId": "4621e75e-f9e8-4714-959f-e60371477463"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21613.000000\n",
       "mean       540.088142\n",
       "std        367.127196\n",
       "min         75.000000\n",
       "25%        321.950000\n",
       "50%        450.000000\n",
       "75%        645.000000\n",
       "max       7700.000000\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To remove outliers first we understand the distribution of our y variable (price)\n",
    "\n",
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18wA32Do5se7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "# We obtain the z-score for our chosen variable, in this case price\n",
    "z = np.abs(stats.zscore(df['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyXUhw_h5_Fl",
    "outputId": "8dcb6b4c-54cf-4cec-c09f-5a24507a7059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date  zipcode      lat     long  bedrooms  bathrooms  \\\n",
      "id                                                                            \n",
      "2524049179  20140826T000000    98040  47.5316 -122.233         3       2.75   \n",
      "7855801670  20150401T000000    98006  47.5620 -122.162         4       3.25   \n",
      "2025069065  20140929T000000    98074  47.6338 -122.072         4       2.50   \n",
      "7960900060  20150504T000000    98004  47.6312 -122.223         4       3.25   \n",
      "7424700045  20150513T000000    98122  47.6166 -122.287         5       3.00   \n",
      "...                     ...      ...      ...      ...       ...        ...   \n",
      "3262300818  20150227T000000    98039  47.6351 -122.236         4       3.75   \n",
      "8964800330  20150407T000000    98004  47.6200 -122.207         4       3.75   \n",
      "715010530   20150113T000000    98006  47.5382 -122.111         5       3.50   \n",
      "524059330   20150130T000000    98004  47.5990 -122.197         4       3.50   \n",
      "9253900271  20150107T000000    98008  47.5943 -122.110         5       4.50   \n",
      "\n",
      "            sqft_above  sqft_basement  sqft_lot  floors  waterfront  \\\n",
      "id                                                                    \n",
      "2524049179        2330            720     44867     1.0           0   \n",
      "7855801670        3540           1640     19850     2.0           0   \n",
      "2025069065        1830           1820      8354     1.0           1   \n",
      "7960900060        4750            300     20100     1.5           0   \n",
      "7424700045        2630           1200      8480     2.0           0   \n",
      "...                ...            ...       ...     ...         ...   \n",
      "3262300818        3290            500      8797     2.0           0   \n",
      "8964800330        4180            910     14823     1.0           0   \n",
      "715010530         2920           1490     13000     2.0           0   \n",
      "524059330         3120            710      8963     2.0           0   \n",
      "9253900271        3540           1310     10584     2.0           1   \n",
      "\n",
      "            condition  yr_built  yr_renovated    price  \n",
      "id                                                      \n",
      "2524049179          3      1968             0  2000.00  \n",
      "7855801670          3      2006             0  2250.00  \n",
      "2025069065          3      2000             0  2400.00  \n",
      "7960900060          3      1982          2008  2900.00  \n",
      "7424700045          5      1905          1994  2050.00  \n",
      "...               ...       ...           ...      ...  \n",
      "3262300818          3      2006             0  1865.00  \n",
      "8964800330          3      2013             0  3000.00  \n",
      "715010530           3      2014             0  1881.58  \n",
      "524059330           3      2014             0  1700.00  \n",
      "9253900271          3      2007             0  3567.00  \n",
      "\n",
      "[406 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "outliers = df[z > threshold]\n",
    "\n",
    "# We print the outliers (we can see we're removing 406 rows out of 21613, approximately 1.8% of the date\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnKO-sW26eYc"
   },
   "outputs": [],
   "source": [
    "df_clean = df[z < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W9BPBiJ7XU6",
    "outputId": "6255d995-a2d4-4ac1-efb4-b56cb10f2111"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.624"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We obtain the first model's prediction strength, without outliers\n",
    "\n",
    "# Linear regression equation #\n",
    "y = df_clean.iloc[:, -1]\n",
    "X_model1 = df_clean.iloc[:, 2:-1]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_model1, y)\n",
    "y_pred_model1 = reg.predict(X_model1)\n",
    "reg.score(X_model1, y).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79piFZpw7ltw",
    "outputId": "230c102e-1b82-4607-9174-577806837988"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.799"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now do the same comparison with the second model, using zipcodes as dummies\n",
    "\n",
    "# Q4. Dummies for the zipcodes #\n",
    "X_model2_1 = df_clean.iloc[:, 2:-1]\n",
    "X_model2_2 = pd.get_dummies(df_clean['zipcode'])\n",
    "X_model2_2.head()\n",
    "X_model2 = pd.concat([X_model2_1, X_model2_2], axis=1)\n",
    "X_model2.shape\n",
    "X_model2 = X_model2.values\n",
    "reg.fit(X_model2, y)\n",
    "y_pred_model2 = reg.predict(X_model2)\n",
    "reg.score(X_model2, y).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mN611ru7u0t"
   },
   "source": [
    "## **Answer Q4: Dropping price outliers**\n",
    "We can see that dropping price outliers (removing values that are more than 3 stds beyond the mean), the first model drops in prediction power while the second model improves in almost 1.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDsRAklN8FvN"
   },
   "outputs": [],
   "source": [
    "# We repeat what we did but now choosing sqft_above as proxy for size\n",
    "#We obtain the z-score for our chosen variable, in this case sqft_above\n",
    "z_size = np.abs(stats.zscore(df['sqft_above']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFiWF1kX8Rxy",
    "outputId": "38c253cc-75ff-4b46-b8f1-14be1ef2a7c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date  zipcode      lat     long  bedrooms  bathrooms  \\\n",
      "id                                                                            \n",
      "7960900060  20150504T000000    98004  47.6312 -122.223         4       3.25   \n",
      "4054500390  20141007T000000    98077  47.7285 -122.042         4       4.75   \n",
      "8678500060  20140710T000000    98024  47.5954 -121.950         5       4.25   \n",
      "2625069070  20150410T000000    98074  47.6179 -122.005         4       3.25   \n",
      "7322910030  20140721T000000    98053  47.6554 -122.018         5       3.50   \n",
      "...                     ...      ...      ...      ...       ...        ...   \n",
      "2625069038  20141124T000000    98074  47.6258 -122.005         4       3.50   \n",
      "324069112   20140617T000000    98075  47.5914 -122.027         4       4.00   \n",
      "1561750040  20141224T000000    98074  47.6018 -122.060         5       4.50   \n",
      "6664500090  20150115T000000    98059  47.4832 -122.145         5       4.00   \n",
      "249000205   20141015T000000    98004  47.6321 -122.200         5       3.75   \n",
      "\n",
      "            sqft_above  sqft_basement  sqft_lot  floors  waterfront  \\\n",
      "id                                                                    \n",
      "7960900060        4750            300     20100     1.5           0   \n",
      "4054500390        5310              0     57346     2.0           0   \n",
      "8678500060        6070              0    171626     2.0           0   \n",
      "2625069070        4860              0    181319     2.5           0   \n",
      "7322910030        4410              0     57063     2.0           0   \n",
      "...                ...            ...       ...     ...         ...   \n",
      "2625069038        4300              0    108865     2.0           0   \n",
      "324069112         4420              0     16526     2.0           0   \n",
      "1561750040        4350              0     13405     2.0           0   \n",
      "6664500090        4500              0      8130     2.0           0   \n",
      "249000205         4470              0      8088     2.0           0   \n",
      "\n",
      "            condition  yr_built  yr_renovated   price  \n",
      "id                                                     \n",
      "7960900060          3      1982          2008  2900.0  \n",
      "4054500390          4      1989             0  1365.0  \n",
      "8678500060          3      1999             0  1550.0  \n",
      "2625069070          3      1993             0  1385.0  \n",
      "7322910030          4      1990             0  1095.0  \n",
      "...               ...       ...           ...     ...  \n",
      "2625069038          3      2014             0  1450.0  \n",
      "324069112           3      2013             0  1325.0  \n",
      "1561750040          3      2014             0  1375.0  \n",
      "6664500090          3      2007             0   750.0  \n",
      "249000205           3      2008             0  1537.0  \n",
      "\n",
      "[254 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "outliers_size = df[z_size > threshold]\n",
    "\n",
    "# We print the outliers (we can see we're removing 254 rows out of 21613, approximately 1.2% of the date\n",
    "print(outliers_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXkKuwX680D4"
   },
   "outputs": [],
   "source": [
    "df_clean_size = df[z_size < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxdZgNnl82KY",
    "outputId": "7e65b5dc-28a2-4126-ad23-fb95607cf1d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.614"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We obtain the first model's prediction strength, without outliers\n",
    "\n",
    "# Linear regression equation #\n",
    "y = df_clean_size.iloc[:, -1]\n",
    "X_model1 = df_clean_size.iloc[:, 2:-1]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_model1, y)\n",
    "y_pred_model1 = reg.predict(X_model1)\n",
    "reg.score(X_model1, y).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twnvaO2H82XH",
    "outputId": "7b3fb164-9edf-4cc5-8b61-7b94f73c971b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.776"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now do the same comparison with the second model, using zipcodes as dummies\n",
    "\n",
    "# Q4. Dummies for the zipcodes #\n",
    "X_model2_1 = df_clean_size.iloc[:, 2:-1]\n",
    "X_model2_2 = pd.get_dummies(df_clean_size['zipcode'])\n",
    "X_model2_2.head()\n",
    "X_model2 = pd.concat([X_model2_1, X_model2_2], axis=1)\n",
    "X_model2.shape\n",
    "X_model2 = X_model2.values\n",
    "reg.fit(X_model2, y)\n",
    "y_pred_model2 = reg.predict(X_model2)\n",
    "reg.score(X_model2, y).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKsHAXyZ9Hz7"
   },
   "source": [
    "## **Answer Q4: Dropping size outliers**\n",
    "We can see that by dropping size outliers (removing values that are more than 3 stds beyond the mean), the first model drops in prediction power and the second model also drops in prediction power.\n",
    "This could be because size is an important variable to keep to avoid overfitting the model to the standard values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgCzQUoN9gFE"
   },
   "source": [
    "## Q5. The distribution of the price is quite skewed, which is a fact of life in real state. The extreme values in the right tail of the distribution can exert an undesired influence on the regression coefficients. Develop and evaluate a model for predicting the price that is based on a linear regression equation which has the logarithm of the price on the left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCx6wi3082fO",
    "outputId": "bd31913a-4422-4d61-d071-9af07dfaa6b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.697"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We obtain the first model's prediction strength\n",
    "\n",
    "# Linear regression equation #\n",
    "df2 = df\n",
    "df2['price'] = np.log(df2['price']) #we obtain log of price\n",
    "y_log = df2['price']\n",
    "X_model1 = df2.iloc[:, 2:-1]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_model1, y)\n",
    "y_pred_model1 = reg.predict(X_model1)\n",
    "reg.score(X_model1, y).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvJrGZrJ-ggA",
    "outputId": "bfeb8cfa-2917-4d96-e320-aac9b461b7bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.837"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now do the same comparison with the second model, using zipcodes as dummies\n",
    "\n",
    "# Q4. Dummies for the zipcodes #\n",
    "X_model2_1 = df2.iloc[:, 2:-1]\n",
    "X_model2_2 = pd.get_dummies(df2['zipcode'])\n",
    "X_model2_2.head()\n",
    "X_model2 = pd.concat([X_model2_1, X_model2_2], axis=1)\n",
    "X_model2.shape\n",
    "X_model2 = X_model2.values\n",
    "reg.fit(X_model2, y_log)\n",
    "y_pred_model2 = reg.predict(X_model2)\n",
    "reg.score(X_model2, y_log).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0ttD20I-tZj"
   },
   "source": [
    "## **Answer Q5:**\n",
    "We can see that by transforming price through its logarithm, both models increase in prediction power. Model 1 goes from 64.6% to 69.7%, while model 2 goes from 78.7% to 83.7%. With these improvements, we can conclude we should consider using the logarithm of price instead."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
